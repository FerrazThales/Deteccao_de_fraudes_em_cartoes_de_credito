{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Detec√ß√£o de Fraude em Cart√µes de Cr√©dito.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FerrazThales/Deteccao_de_fraudes_em_cartoes_de_credito/blob/main/Detec%C3%A7%C3%A3o_de_Fraude_em_Cart%C3%B5es_de_Cr%C3%A9dito.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sC2BFMOKN1M7"
      },
      "source": [
        "---\n",
        "<img width=\"32%\" src=\"https://github.com/FerrazThales/FerrazThales/blob/main/logo_gif.gif?raw=true\">\n",
        "\n",
        "[Thales Ferraz üë®‚Äçüíª | Data Scientist](https://thalesferraz.medium.com/)\n",
        "\n",
        "* ***Visite tamb√©m o meu artigo sobre esta an√°lise no [Medium](https://thalesferraz.medium.com/o-que-os-dados-nos-dizem-sobre-a-covid-19-96a2a7a984f4)***\n",
        "\n",
        "* ***Visite mais projetos do meu portf√≥lio no [Github do Thales Ferraz](https://bit.ly/3DQyZHu)***\n",
        "\n",
        "* ***Vamos trocar mais id√©ias sobre Data Science no [LinkedIn](https://www.linkedin.com/in/thalesdefreitasferraz/)?***\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IR56trFcPcC1"
      },
      "source": [
        "# Detec√ß√£o de Fraudes em Cart√µes de Cr√©dito\n",
        "\n",
        "Neste projeto, iremos abordar o problema das fraudes em cart√µes de cr√©dito, uma das principais preocupa√ß√µes das institui√ß√µes financeiras como bancos e *fintechs*. Apenas no Brasil, cerca de 12,1 milh√µes de pessoas j√° foram v√≠timas de algum tipo de fraude financeira no √∫ltimo ano. Traduzindo em valores, os golpes financeiros ultrapassaram a cifra de R$ 1,8 bilh√£o de preju√≠zo por ano para os √∫ltimos 12 meses.\n",
        "\n",
        "<p align=center>\n",
        "<img src=\"https://images.unsplash.com/photo-1592772874383-d08932d29db7?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=699&q=80\" width=\"60%\"></p>\n",
        "\n",
        "Dentra essas fraudes, aquelas envolvendo cart√µes de cr√©dito s√£o de grande relev√¢ncia uma vez que a sua n√£o-detec√ß√£o acarretar√° em preju√≠zos consider√°veis, tanto para o consumidor quanto para a institui√ß√£o financeira.\n",
        "\n",
        "Um outro fator a ser considerado √© a quantidade de *falsos positivos*, ou seja, aquelas vezes em que voc√™ tentou fazer uma compra e teve seu cart√£o bloqueado preventivamente - o que provavelmente gerou estresse e constrangimento.\n",
        "\n",
        "Por todos esses motivos, o investimento na √°rea de detec√ß√£o de fraudes por meio de Intelig√™ncia Artificial vem crescendo a cada ano ??????????? (quanto?), representando uma grande oportunidade em *Data Science*. \n",
        "\n",
        "Dispondo de grandes volumes de dados como base hist√≥rica, um algoritmo de machine learning apenas um pouco melhor que os anteriores j√° representa uma economia de milh√µes de Reais. E esse √© o desafio, aprimorar cada vez mais o uso de algoritmos visando inibir ou evitar transa√ß√µes fraudulentas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HVmpIxQWT4Y"
      },
      "source": [
        "## Importando os Dados\n",
        "\n",
        "Os dados que usaremos neste projeto foram disponibilizados por algumas empresas europ√©ias de cart√£o de cr√©dito. O *dataset* representa as opera√ß√µes financeiras que aconteceram no per√≠odo de dois dias, onde foram classificadas 492 fraudes em meio a quase 290 mil transa√ß√µes.\n",
        "\n",
        "Como voc√™ pode notar, este √© um conjunto de dados extremamente desbalanceado, onde as fraudes representam apenas 0,17% do total.\n",
        "\n",
        "Outro detalhe interessante √© que as *features* s√£o todas num√©ricas, e foram descaracterizadas (por problemas ligados √† privacidade e seguran√ßa). Assim, os nomes das colunas s√£o representados por $[V1, V2, V3 \\dots, V28]$ \n",
        "\n",
        "<p align=center>\n",
        "<img src=\"https://images.unsplash.com/photo-1620714223084-8fcacc6dfd8d?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1051&q=80\" width=\"50%\"></p>\n",
        "\n",
        "[Na p√°gina original dos dados](https://www.kaggle.com/mlg-ulb/creditcardfraud), tamb√©m √© informado que as vari√°veis passaram por uma transforma√ß√£o conhecida como An√°lise de Componentes Principais (*Principal Component Analysis* - PCA).\n",
        "\n",
        "* link para PCA\n",
        "\n",
        "A PCA permite a redu√ß√£o da dimensionalidade enquanto mant√©m o maior n√∫mero poss√≠vel de informa√ß√µes. Para conseguir isso, o algoritmo encontra um conjunto novo de recursos - os chamados **componentes**.\n",
        "\n",
        "Esses componentes s√£o em n√∫mero menor or igual √†s vari√°veis originais. No caso deste projeto, os componentes achados pela transforma√ß√£o da PCA s√£o as pr√≥prias colunas $[V1, V2, V3 \\dots, V28]$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQ4bNy7udtEE"
      },
      "source": [
        "# importar os pacotes necess√°rios\n",
        "import pandas as pd"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NxUOfDOj2j8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "outputId": "925b49b0-b7e9-4f1a-ede1-ecbf8c860c28"
      },
      "source": [
        "file_path = \"https://www.dropbox.com/s/b44o3t3ehmnx2b7/creditcard.csv?dl=1\"\n",
        "endereco = 'https://www.kaggle.com/mlg-ulb/creditcardfraud/download'\n",
        "df = pd.read_csv(endereco)\n",
        "df.head()\n",
        "# importar os dados para um dataframe\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-2229543c1513>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://www.dropbox.com/s/b44o3t3ehmnx2b7/creditcard.csv?dl=1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mendereco\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://www.kaggle.com/mlg-ulb/creditcardfraud/download'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendereco\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# importar os dados para um dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2157\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2158\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 7, saw 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nF_Dhd81Cvq"
      },
      "source": [
        "Com os dados importados para dentro de uma estrutura *Dataframe* - e n√£o havendo a necessidade de mais nenhum ajuste ou configura√ß√£o nesta etapa, pode-se iniciar uma an√°lise explorat√≥ria dos dados a fim de preparar um modelo de *Machine Learning*.\n",
        "\n",
        "Lembre-se de realizar o *split* dos conjuntos, para evitar o vazamento de dados.\n",
        "\n",
        "Feito isso, vamos √† An√°lise Explorat√≥ria."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UtXyZ6stlSM"
      },
      "source": [
        "## An√°lise Explorat√≥ria\n",
        "\n",
        "Abordar t√≥picos/informa√ß√µes como:\n",
        "\n",
        "* Ver as 5 primeiras entradas\n",
        "* Ver o resumo estat√≠stico do dataframe\n",
        "* Verificar se h√° valores ausentes\n",
        "* Plotar um gr√°fico de barras (ou countplot) para ver o balanceamento do *dataset*\n",
        "* Plotar os seguintes histogramas:\n",
        "    * Vari√°vel `Time`\n",
        "        * Fraude (`Class == 1`)\n",
        "        * Normal (`Class == 0`)\n",
        "    * Vari√°vel `Amount`\n",
        "        * Fraude (`Class == 1`)\n",
        "        * Normal (`Class == 0`)\n",
        "* Plotar um `boxplot` para a vari√°vel `Amount` quando houve fraude (`Class == 1`)\n",
        "* Plotar uma matriz de correla√ß√£o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULe7z0jZt0EH"
      },
      "source": [
        "## Prepara√ß√£o dos Dados\n",
        "\n",
        "* Normalizar os dados que ainda n√£o haviam sido pr√©-processados (`Time` e `Amount`)\n",
        "* Dividir o conjunto de dados entre treino e valida√ß√£o\n",
        "* [*Recomendado*] Balancear o conjunto de dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqfjG_SUSTi-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJNH5qcjRxfX"
      },
      "source": [
        "## Modelo de Machine Learning\n",
        "\n",
        "* Construir um modelo para **classifica√ß√£o**.\n",
        "* [*Opcional*] Construir mais de um modelo para avaliar os desempenhos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDySx8XUSMw_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e4ENOTYSUXi"
      },
      "source": [
        "## Avaliar o desempenho do modelo\n",
        "\n",
        "* Identificar a melhor m√©trica para esse tipo de modelo\n",
        "* [*Opcional*] Comparar o desempenho entre diversos modelos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1jEi7gkSe2r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Bj7zRZMSfO7"
      },
      "source": [
        "## Conclus√£o\n",
        "\n",
        "* Escrever suas conclus√µes a respeito da constru√ß√£o do modelo"
      ]
    }
  ]
}